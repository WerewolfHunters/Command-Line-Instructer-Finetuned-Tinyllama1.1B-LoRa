# ğŸ§  Command-Line Instruction Agent (LoRA Fine-Tuning)

This project showcases an **end-to-end mini-demo** of fine-tuning a small open-weight language model on a curated dataset of CLI-based Q&A pairs using **LoRA**, and deploying it as a command-line agent that generates executable shell plans.

---

## ğŸš€ Features

- âœ… 200+ command-line Q&A pairs (Bash, Git, tar/gzip, grep, venv, etc.)
- âœ… LoRA fine-tuning on `TinyLlama-1.1B` using free Google Colab (T4 GPU)
- âœ… `agent.py`: terminal-based agent that:
  - Accepts instructions like `"Create a Git branch"`
  - Generates a shell-based step-by-step plan
  - Performs a dry-run (`echo <command>`)
  - Logs execution to `logs/trace.jsonl`
- âœ… Static & dynamic evaluation with BLEU/ROUGE and plan quality scoring

---

## ğŸ—‚ï¸ Project Structure

â”œâ”€â”€ agent.py # CLI agent script<br>
â”œâ”€â”€ data/<br>
â”‚ â””â”€â”€ qa_pairs.json # â‰¥ 150 Q&A pairs for fine-tuning<br>
â”œâ”€â”€ logs/<br>
â”‚ â””â”€â”€ trace.jsonl # Dry-run logs from agent<br>
â”œâ”€â”€ lora_adapter/ # LoRA adapter weights after fine-tuning<br>
â”œâ”€â”€ preprocess_data.py # Script to generate dataset<br>
â”œâ”€â”€ finetune_lora_colab.ipynb # Colab notebook for training<br>
â”œâ”€â”€ eval_static.md # Static model evaluation<br>
â”œâ”€â”€ eval_dynamic.md # Dynamic evaluation with scores<br>
â”œâ”€â”€ report.md # One-page project report<br>
â””â”€â”€ README.md # This file<br>
